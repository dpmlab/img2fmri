{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushes input images through our pretrained resnet18 model and saves the activations.\n",
    "# Can be easily modified to a different network or layer if desired.\n",
    "def generate_activations(input_dir, output_dir=\"\"):\n",
    "    if output_dir == \"\": \n",
    "        output_dir = f\"temp/activations/\"\n",
    "\n",
    "    # Default input image transformations for ImageNet\n",
    "    scaler = transforms.Resize((224, 224))\n",
    "#     scaler = transforms.CenterCrop((224, 224))\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    to_tensor = transforms.ToTensor()\n",
    "\n",
    "    # Load our pretrained model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    for filename in tqdm(glob.glob(f\"{input_dir}/*\"), desc='Pushing images through CNN'):\n",
    "        if Path(filename).suffix not in [\".jpg\", '.JPG', '.jpeg', '.JPEG', \".png\", '.PNG']:\n",
    "            continue\n",
    "\n",
    "        img = Image.open(filename)\n",
    "        t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
    "\n",
    "        # Create network up to last layer and push image through\n",
    "        layer_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        feature_vec = layer_extractor(t_img).data.numpy().squeeze()\n",
    "        feature_vec = feature_vec.flatten()\n",
    "\n",
    "        # Save image activations\n",
    "        image_name = Path(filename).stem\n",
    "        np.save(f\"{output_dir}/{image_name}.npy\", feature_vec)\n",
    "        img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'cnn_activations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract stimuli data for training models (341MB)\n",
    "# output is directory 'presented_stimuli' with subdirectories for each dataset\n",
    "stimuli_url = \"https://figshare.com/ndownloader/files/34907709?private_link=04a31f18105e18f24b7f\"\n",
    "input_dir = 'presented_stimuli'\n",
    "filename = wget.download(stimuli_url)\n",
    "\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing images through CNN: 100%|██████████| 4916/4916 [07:05<00:00, 11.54it/s]\n",
      "Pushing images through CNN: 100%|██████████| 2000/2000 [02:54<00:00, 11.45it/s]\n",
      "Pushing images through CNN: 100%|██████████| 1916/1916 [02:49<00:00, 11.32it/s]\n",
      "Pushing images through CNN: 100%|██████████| 1000/1000 [01:26<00:00, 11.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Push images through CNN\n",
    "for folder in glob.glob(\"presented_stimuli/*\"):\n",
    "    generate_activations(folder, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads CNN activations previously saved in specified directory\n",
    "def load_activations(activations_folder):\n",
    "    stimuli_list = np.load('stimuli_list.pkl', allow_pickle=True)\n",
    "    activations = []\n",
    "    \n",
    "    num_images = len(glob.glob(f'{activations_folder}/*'))\n",
    "    with tqdm(total=num_images, desc='Loading activations') as pbar:\n",
    "        for image_name in stimuli_list:\n",
    "            for filename in glob.glob(f'{activations_folder}/{Path(image_name).stem}.npy'):\n",
    "                img_activation = np.load(filename, allow_pickle = True)\n",
    "                activations.append(img_activation)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return np.asarray(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading activations: 100%|██████████| 4916/4916 [00:01<00:00, 3334.55it/s]\n"
     ]
    }
   ],
   "source": [
    "activations = load_activations(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bold5000_reordered_data.npy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download fMRI data for training models (282MB)\n",
    "fmri_url = \"https://figshare.com/ndownloader/files/34907763?private_link=97d3168c53e76e28a072\"\n",
    "wget.download(fmri_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_raw = np.load('bold5000_reordered_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange fmri data to num_rois x num_subjects x num_voxels x num_samples\n",
    "fmri_preprocessed = np.empty((5,3,), dtype=object)\n",
    "for roi_idx in range(5):\n",
    "    for sub_idx in range(3):\n",
    "        sub_roi = np.vstack(fmri_raw[sub_idx][roi_idx]).T\n",
    "        sub_roi = stats.zscore(sub_roi, axis=1)\n",
    "        fmri_preprocessed[roi_idx, sub_idx] = np.asarray(sub_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI: EarlyVis for subject1 saved\n",
      "ROI: EarlyVis for subject2 saved\n",
      "ROI: EarlyVis for subject3 saved\n",
      "ROI: OPA for subject1 saved\n",
      "ROI: OPA for subject2 saved\n",
      "ROI: OPA for subject3 saved\n",
      "ROI: LOC for subject1 saved\n",
      "ROI: LOC for subject2 saved\n",
      "ROI: LOC for subject3 saved\n",
      "ROI: RSC for subject1 saved\n",
      "ROI: RSC for subject2 saved\n",
      "ROI: RSC for subject3 saved\n",
      "ROI: PPA for subject1 saved\n",
      "ROI: PPA for subject2 saved\n",
      "ROI: PPA for subject3 saved\n"
     ]
    }
   ],
   "source": [
    "# Note that although we save models for all five ROIs specified with the \n",
    "# BOLD5000 dataset, we end up only using LOC, RSC, and PPA in our analyses\n",
    "roi_list = [\"EarlyVis\",\"OPA\", \"LOC\", \"RSC\", \"PPA\"]\n",
    "\n",
    "ridge_p_grid = {'alpha': np.logspace(1, 5, 10)}\n",
    "save_location = f\"models/\"\n",
    "\n",
    "for roi_idx, roi in enumerate(roi_list):\n",
    "    for subj in range(3):\n",
    "\n",
    "        X_train = activations\n",
    "        y_train = fmri_preprocessed[roi_idx, subj].T\n",
    "\n",
    "        grid = GridSearchCV(Ridge(), ridge_p_grid)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        pkl_filename = f'{save_location}subj{subj+1}_{roi_list[roi_idx]}_model.pkl'\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(grid.best_estimator_, file)\n",
    "            \n",
    "        print(f\"ROI: {roi_list[roi_idx]} for subject{subj+1} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
