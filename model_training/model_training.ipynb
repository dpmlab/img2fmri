{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - comments\n",
    "# - directory handling?\n",
    "# - uploading stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will only work once pip'd\n",
    "from imgtofmri import generate_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activations(input_dir, output_dir=\"\"):\n",
    "    if output_dir == \"\": \n",
    "        output_dir = f\"temp/activations/\"\n",
    "\n",
    "    # Default input image transformations for ImageNet\n",
    "    scaler = transforms.Resize((224, 224))\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    to_tensor = transforms.ToTensor()\n",
    "\n",
    "    # Load the pretrained model, set to eval mode\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    for filename in tqdm(glob.glob(f\"{input_dir}/*\"), desc='Pushing images through CNN'):\n",
    "        # TODO need to have a check for non jpg/pngs... should just have a try except probs\n",
    "        if Path(filename).suffix not in [\".jpg\", '.JPG', '.jpeg', '.JPEG', \".png\", '.PNG']:\n",
    "            print(f\"skipping {filename} with suffix: {Path(filename).suffix}\")\n",
    "            continue\n",
    "\n",
    "        img = Image.open(filename)\n",
    "        t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
    "\n",
    "        # Create network up to last layer, push image through, flatten\n",
    "        layer_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        feature_vec = layer_extractor(t_img).data.numpy().squeeze()\n",
    "        feature_vec = feature_vec.flatten()\n",
    "\n",
    "        # Save image\n",
    "        image_name = Path(filename).stem\n",
    "        np.save(f\"{output_dir}/{image_name}.npy\", feature_vec)\n",
    "        img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'cnn_activations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing images through CNN: 100%|██████████| 2000/2000 [02:57<00:00, 11.27it/s]\n",
      "Pushing images through CNN: 100%|██████████| 1916/1916 [02:50<00:00, 11.21it/s]\n",
      "Pushing images through CNN: 100%|██████████| 1000/1000 [01:26<00:00, 11.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for folder in glob.glob(\"presented_stimuli/*\"):\n",
    "    generate_activations(folder, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activations(activations_folder):\n",
    "    stimuli_list = np.load('stimuli_list.pkl', allow_pickle=True)\n",
    "    activations = []\n",
    "    \n",
    "    num_images = len(glob.glob(f'{activations_folder}/*'))\n",
    "    with tqdm(total=num_images, desc='Loading activations') as pbar:\n",
    "        for image_name in stimuli_list:\n",
    "            for filename in glob.glob(f'{activations_folder}/{Path(image_name).stem}.npy'):\n",
    "                img_activation = np.load(filename, allow_pickle = True)\n",
    "                activations.append(img_activation)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return np.asarray(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading activations: 100%|██████████| 4916/4916 [00:03<00:00, 1490.62it/s]\n"
     ]
    }
   ],
   "source": [
    "activations = load_activations(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_raw = np.load('bold5000_reordered_data.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_preprocessed = np.empty((5,3,), dtype=object)\n",
    "for roi_idx in range(5):\n",
    "    for sub_idx in range(3):\n",
    "        sub_roi = np.vstack(fmri_raw[sub_idx][roi_idx])\n",
    "        sub_roi = np.transpose(sub_roi)\n",
    "        sub_roi = stats.zscore(sub_roi, axis=1)\n",
    "        fmri_preprocessed[roi_idx, sub_idx] = np.asarray(sub_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is now in shape: num_rois x num_subjects x num_voxels x num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI: EarlyVis for subject1 saved\n",
      "ROI: EarlyVis for subject2 saved\n",
      "ROI: EarlyVis for subject3 saved\n",
      "ROI: OPA for subject1 saved\n",
      "ROI: OPA for subject2 saved\n",
      "ROI: OPA for subject3 saved\n",
      "ROI: LOC for subject1 saved\n",
      "ROI: LOC for subject2 saved\n",
      "ROI: LOC for subject3 saved\n",
      "ROI: RSC for subject1 saved\n",
      "ROI: RSC for subject2 saved\n",
      "ROI: RSC for subject3 saved\n",
      "ROI: PPA for subject1 saved\n",
      "ROI: PPA for subject2 saved\n",
      "ROI: PPA for subject3 saved\n"
     ]
    }
   ],
   "source": [
    "roi_list = [\"EarlyVis\",\"OPA\", \"LOC\", \"RSC\", \"PPA\"]\n",
    "\n",
    "ridge_p_grid = {'alpha': np.logspace(1, 5, 10)}\n",
    "save_location = f\"models/\"\n",
    "\n",
    "for roi_idx, roi in enumerate(roi_list):\n",
    "    for subj in range(3):\n",
    "\n",
    "        X_train = activations\n",
    "#         y_train = np.vstack(fmri_preprocessed[roi_idx, subj]).T\n",
    "        y_train = fmri_preprocessed[roi_idx, subj].T\n",
    "\n",
    "        grid = GridSearchCV(Ridge(), ridge_p_grid)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        pkl_filename = f'{save_location}subj{subj+1}_{roi_list[roi_idx]}_model.pkl'\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(grid.best_estimator_, file)\n",
    "        print(f\"ROI: {roi_list[roi_idx]} for subject{subj+1} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
