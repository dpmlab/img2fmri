{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCVz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "TQDM_FORMAT = '{l_bar}{bar:10}{r_bar}{bar:-10b}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activations(input_dir, output_dir=join('temp','activations'), center_crop=False):\n",
    "    \"\"\"Pushes input images through our pretrained resnet18 model and saves the activations.\n",
    "    Can be modified to use a different network or layer if desired, by changing 'model'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        Relative path to input directory of images to be predicted.\n",
    "    output_dir : str\n",
    "        Relative path to save intermediate files used in prediction.\n",
    "        Defaults to 'temp/activations/'.\n",
    "    center_crop : bool\n",
    "        If True, crops each image to a square, from the center of the\n",
    "        image, before processing. If False, images are resized to a square before processing.\n",
    "        Defaults to False.\n",
    "    \"\"\"\n",
    "    # Default input image transformations for ImageNet\n",
    "    if center_crop:\n",
    "        scaler = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop((224, 224))\n",
    "        ])\n",
    "    else:\n",
    "        scaler = transforms.Resize((224, 224))\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    to_tensor = transforms.ToTensor()\n",
    "\n",
    "    # Load our pretrained model. NOTE: this is where the model could be extended or feature \n",
    "    # extraction model changed.\n",
    "    model = models.resnet18(weights='DEFAULT')\n",
    "    model.eval()\n",
    "\n",
    "    desc = 'Pushing images through CNN'\n",
    "    for filename in tqdm(glob.glob(join(input_dir,'*')), bar_format=TQDM_FORMAT, desc=desc):\n",
    "        if Path(filename).suffix not in ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']:\n",
    "            continue\n",
    "\n",
    "        img = Image.open(filename)\n",
    "        t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
    "\n",
    "        # Create network up to last layer and push image through\n",
    "        layer_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        feature_vec = layer_extractor(t_img).data.numpy().squeeze()\n",
    "        feature_vec = feature_vec.flatten()\n",
    "\n",
    "        # Save image activations\n",
    "        image_name = Path(filename).stem\n",
    "        np.save(join(output_dir,f'{image_name}.npy'), feature_vec)\n",
    "        img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'cnn_activations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................................] 341219259 / 341219259"
     ]
    }
   ],
   "source": [
    "# Download and extract stimuli data for training models (341MB)\n",
    "# output is directory 'presented_stimuli' with subdirectories for each dataset\n",
    "stimuli_url = \"https://figshare.com/ndownloader/files/36563031\"\n",
    "input_dir = 'presented_stimuli'\n",
    "filename = wget.download(stimuli_url)\n",
    "\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing images through CNN: 100%|██████████| 2000/2000 [02:20<00:00, 14.23it/s]                                                                                                     \n",
      "Pushing images through CNN: 100%|██████████| 1916/1916 [02:18<00:00, 13.80it/s]                                                                                                     \n",
      "Pushing images through CNN: 100%|██████████| 1000/1000 [01:12<00:00, 13.73it/s]                                                                                                     \n"
     ]
    }
   ],
   "source": [
    "# Push images through CNN\n",
    "for folder in glob.glob(\"presented_stimuli/*\"):\n",
    "    generate_activations(folder, output_dir, center_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads CNN activations previously saved in specified directory\n",
    "def load_activations(activations_folder):\n",
    "    stimuli_list = np.load('stimuli_list.pkl', allow_pickle=True)\n",
    "    activations = []\n",
    "    \n",
    "    num_images = len(glob.glob(f'{activations_folder}/*'))\n",
    "    with tqdm(total=num_images, bar_format=TQDM_FORMAT, desc='Loading activations') as pbar:\n",
    "        for image_name in stimuli_list:\n",
    "            for filename in glob.glob(f'{activations_folder}/{Path(image_name).stem}.npy'):\n",
    "                img_activation = np.load(filename, allow_pickle = True)\n",
    "                activations.append(img_activation)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return np.asarray(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading activations: 100%|██████████| 4916/4916 [00:01<00:00, 3893.31it/s]                                                                                                          \n"
     ]
    }
   ],
   "source": [
    "activations = load_activations(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................................] 281878296 / 281878296"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bold5000_reordered_data.npy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download fMRI data for training models (282MB)\n",
    "fmri_url = \"https://figshare.com/ndownloader/files/34907763\"\n",
    "wget.download(fmri_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_raw = np.load('bold5000_reordered_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange fmri data to num_rois x num_subjects x num_voxels x num_samples\n",
    "fmri_preprocessed = np.empty((5,3,), dtype=object)\n",
    "for roi_idx in range(5):\n",
    "    for sub_idx in range(3):\n",
    "        sub_roi = np.vstack(fmri_raw[sub_idx][roi_idx]).T\n",
    "        sub_roi = stats.zscore(sub_roi, axis=1)\n",
    "        fmri_preprocessed[roi_idx, sub_idx] = np.asarray(sub_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI: EarlyVis for subject1 saved\n",
      "ROI: EarlyVis for subject2 saved\n",
      "ROI: EarlyVis for subject3 saved\n",
      "ROI: OPA for subject1 saved\n",
      "ROI: OPA for subject2 saved\n",
      "ROI: OPA for subject3 saved\n",
      "ROI: LOC for subject1 saved\n",
      "ROI: LOC for subject2 saved\n",
      "ROI: LOC for subject3 saved\n",
      "ROI: RSC for subject1 saved\n",
      "ROI: RSC for subject2 saved\n",
      "ROI: RSC for subject3 saved\n",
      "ROI: PPA for subject1 saved\n",
      "ROI: PPA for subject2 saved\n",
      "ROI: PPA for subject3 saved\n"
     ]
    }
   ],
   "source": [
    "# Note that although we save models for all five ROIs specified with the \n",
    "# BOLD5000 dataset, we end up only using LOC, RSC, and PPA in our analyses\n",
    "roi_list = [\"EarlyVis\",\"OPA\", \"LOC\", \"RSC\", \"PPA\"]\n",
    "\n",
    "ridge_p_grid = {'alpha': np.logspace(1, 5, 10)}\n",
    "save_location = f\"models/\"\n",
    "\n",
    "for roi_idx, roi in enumerate(roi_list):\n",
    "    for subj in range(3):\n",
    "\n",
    "        X_train = activations\n",
    "        y_train = fmri_preprocessed[roi_idx, subj].T\n",
    "\n",
    "        grid = GridSearchCV(Ridge(), ridge_p_grid)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        pkl_filename = f'{save_location}subj{subj+1}_{roi_list[roi_idx]}_model.pkl'\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(grid.best_estimator_, file)\n",
    "            \n",
    "        print(f\"ROI: {roi_list[roi_idx]} for subject{subj+1} saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.9",
   "language": "python",
   "name": "test3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
